[{"content":"I recently migrated a Next.js project to Remix in order to improve performance and maintainability. These are the good parts and the bad parts of Remix.\ntl;dr The migrated Next.js app can be described as follows:\n10 file based routes 50 components Tailwind for styling deployed on Vercel After the migration to Remix and using Cloudflare Workers as deployment target, following changed:\n10-15% performance improvement (Core Web Vitals) 5-10% smaller code base The migration itself was mostly straightforward with minor hiccups around SSR third-party React components.\nThe migration The Next.js app had a clean separation of pages and components. Each component contained its own style and logic. Some of the logic was extracted using hooks for easier re-use across multiple components.\nBoth Next.js and Remix are React frameworks with decent support for Tailwind. After setting up the initial structure, migrating the React components was as easy as going through all of their dependencies and replacing every package with a @next scope.\nImages Next.js provides a solid \u0026lt;Image\u0026gt; component that works nicely out of the box. The component does quiet a bit of heavy lifting and I only realized its scope when I had to find a replacement. Remix does not come with a comparable solution. At the time of writing, there is remix-image.\n\u0026lt;Image src=\u0026#34;https://asset.example.org/image.png\u0026#34; responsive={[ { size: { width: 100, height: 100 }, maxWidth: 500, }, { size: { width: 600, height: 600 }, }, ]} dprVariants={[1, 3]} /\u0026gt; I decided to use the available asset service of the client to create renditions and setting srcset manually. I ended up with a smaller, less powerful, custom version of Next.js\u0026rsquo; \u0026lt;Image\u0026gt;.\nRouting Both Remix and Next.js have file based routing. You create a path hierarchy by creating files that export React components (and other things).\nBoth frameworks come with their \u0026lt;Link\u0026gt; components to abstract away client side routing and data fetching. Replacing the Next.js version with the Remix version was trivial.\nRemix brings nested routes to the table. Strictly speaking, the router of Remix can do everything that Next.js can do. It should be possible to keep the routes and the file structure of a Next.js app and just keep using them in the Remix app.\nHowever, in order to take advantage of proper error propagation and better data reading for child pages, I switched to Remix\u0026rsquo;s \u0026lt;Outlet\u0026gt; for nested pages. Outlets are a way of telling a component \u0026ldquo;this is the place where your children will be rendered\u0026rdquo; inside you. The children pages bring their own loader and action functions. They know how to read their own data and they provide the logic to handle data writes such as form submissions.\nThis allowed me to replace custom client-server HTTP POST code with fetcher or even \u0026lt;Form\u0026gt;. Remix gives you \u0026ldquo;loading\u0026rdquo; or \u0026ldquo;pending\u0026rdquo; states for free, no need to do that manually.\nconst [isLoading, setIsLoading] = useState\u0026lt;boolean\u0026gt;(false) const [isSubmitted, setIsSubmitted] = useState\u0026lt;boolean\u0026gt;(false) useEffect(() =\u0026gt; { if (isSubmitted) { setIsLoading(true) // submit here .finally(() =\u0026gt; setIsLoading(false)) setIsLoading(false) } }, [isSubmitted]) SSR The most painful part of the migration was SSR with third-party components (= React components from NPM). On the client, both Next.js and Remix are just running React. Consequentially, what worked on the client on Next.js works on Remix.\nRendering on the server is a different story, and with Remix it\u0026rsquo;s not an easy one. Many third party React components have a section about SSR. Unfortunately, the instructions are valid for Next.js, maybe sometimes for Gatsby. It seems that Remix is not yet a first-class React framework among React developers.\nThe component that I am still not able to render on the server is react-select because of its use of emotion. A workaround using \u0026lt;ClientOnly /\u0026gt; from remix-utils did the trick.\nimport Select from \u0026#34;react-select\u0026#34; import { ClientOnly } from \u0026#34;remix-utils\u0026#34; const CustomDropdown = () =\u0026gt; { return ( \u0026lt;ClientOnly\u0026gt; \u0026lt;Select /\u0026gt; \u0026lt;/ClientOnly\u0026gt; ) } It is important to render lace holders or empty components to avoid layout shift when the client-side rendering kicks in. In this case, the width depends on the rendered data. Hard coding w-64 fixed the issue.\nimport Select from \u0026#34;react-select\u0026#34; import { ClientOnly } from \u0026#34;remix-utils\u0026#34; const CustomDropdown = () =\u0026gt; { return ( \u0026lt;div className=\u0026#34;w-64\u0026#34;\u0026gt; \u0026lt;ClientOnly\u0026gt; \u0026lt;Select /\u0026gt; \u0026lt;/ClientOnly\u0026gt; \u0026lt;/div\u0026gt; ) } Remix vs. Next.js Remix pros Nested routes: A powerful abstraction that allows the framework to optimize data fetching and provide an excellent DX by making error handling simpler Data writes: Remix provides mechanisms to submit forms and to do other data writes Use the platform: There is a certain simplicity in using what is provided by the browser. The community seems to favor copying and adjusting snippets over adding another dependency. I see similarities to the Go community, where people say \u0026ldquo;a little bit of duplication is better than an additional dependency\u0026rdquo;. The official Remix Stacks are project templates that you adjust to your needs. There is no meta Remix framework that tries to abstract things away for you. Faster: Remix is 10-15% faster with the same code base. Keep in mind that the two deployment targets are different. Next.js was deployed to Vercel and Remix is deployed to Cloudflare Workers. Performance testing was done using WebPageTest.org, web.dev and Lighthouse. Remix cons Small ecosystem: Next.js has better support in the React SSR ecosystem. Helper components like \u0026lt;Image\u0026gt; are part of Next.js, while Remix does not officially have that component. Little Q\u0026amp;A material: The community is smaller and most of the answers are provided on the Remix Discord, which is not indexed by Google or Bing. Whenever I encounter issues, I run a query on Google and another one on Discord. No specialized hosting platform: Remix is built to be runtime agnostic, so it runs on the edge or on Node.js. The documentation and the community recommend to deploy Remix to fly.io or Cloudflare. However, there is no integrated solution like Vercel that is specialized in hosting Remix apps. Verdict Both are great frameworks that make developing React apps a joy.\nRemix is younger and more cutting edge. The documentation is good, but if things go wrong you better be ready to hop on the Discord or read the source code (which is a good idea anyway 😛).\nNested routes and a unified way to read and write data is a big step into the right direction.\nAt the time of writing, the ecosystem is trying to figure out how to build cool things on top of the novel but solid abstractions that Remix provides. Some third-party React components that work flawlessly with Next.js are painful to use with Remix SSR.\n","permalink":"https://www.joseferben.com/posts/migrating-from-nextjs-to-remix/","summary":"\u003cp\u003eI recently migrated a Next.js project to Remix in order to improve performance and maintainability. These are the good parts and the bad parts of Remix.\u003c/p\u003e","title":"Migrating from NextJS to Remix"},{"content":"This year I started consolidating all the major modes by using lsp-mode and apheleia for all the programming language I use. The switch from OCaml\u0026rsquo;s merlin-mode was surprisingly painless.\nLet\u0026rsquo;s start with the default major modes that provide syntax highlighting and a few other things. I have tuareg for OCaml, dune-format to format dune files and reason-mode for ReasonML.\n(use-package tuareg :ensure t :custom (tuareg-opam-insinuate t) :config) (use-package dune-format :ensure t) (use-package reason-mode :ensure t) My lsp-mode setup is the same for every language so I get a consistent experience. I am using ocamllsp as the LSP server implementation, this is the one that the official VS Code extension uses as well.\n(defun my-lsp-fix-buffer () \u0026#34;Formats buffer and organizes imports.\u0026#34; (interactive) (lsp-organize-imports) (lsp-format-buffer)) (use-package lsp-mode :ensure t :after flycheck :commands lsp :bind ((\u0026#34;C-c l n\u0026#34; . flycheck-next-error) (\u0026#34;C-c l d\u0026#34; . lsp-find-definition) (\u0026#34;C-c l r\u0026#34; . lsp-find-references) (\u0026#34;C-c l h\u0026#34; . lsp-describe-thing-at-point) (\u0026#34;C-c l i\u0026#34; . lsp-find-implementation) (\u0026#34;C-c l R\u0026#34; . lsp-rename) (\u0026#34;C-c l o\u0026#34; . my-lsp-fix-buffer)) :hook ((tuareg-mode . lsp) (caml-mode . lsp) (reason-mode . lsp) (before-save . lsp-organize-imports)) :custom (lsp-lens-enable t) (lsp-log-io nil) (lsp-headerline-breadcrumb-enable nil) :config (lsp-enable-which-key-integration t) (lsp-register-client (make-lsp-client :new-connection (lsp-stdio-connection \u0026#39;(\u0026#34;opam\u0026#34; \u0026#34;exec\u0026#34; \u0026#34;--\u0026#34; \u0026#34;ocamllsp\u0026#34;)) :major-modes \u0026#39;(caml-mode tuareg-mode reason-mode) :server-id \u0026#39;ocamllsp))) For my formatting needs I use the outstanding apheleia package. This package is great, especially for slow formatters. With ocamlformat you won\u0026rsquo;t get all the benefits because it\u0026rsquo;s quite fast.\nApheleia runs the formatting on a buffer in the background and patches the current buffer asynchronously. The cursor remains at the same location in the code, which makes it easy to keep track of what is going on.\n(use-package apheleia :ensure t :hook (caml-mode . apheleia-mode) (tuareg-mode . apheleia-mode) (reason-mode . apheleia-mode) :config (setf (alist-get \u0026#39;ocamlformat apheleia-formatters) \u0026#39;(\u0026#34;opam\u0026#34; \u0026#34;exec\u0026#34; \u0026#34;--\u0026#34; \u0026#34;ocamlformat\u0026#34; \u0026#34;--impl\u0026#34; \u0026#34;-\u0026#34;)) (setf (alist-get \u0026#39;refmt apheleia-formatters) \u0026#39;(\u0026#34;opam\u0026#34; \u0026#34;exec\u0026#34; \u0026#34;--\u0026#34; \u0026#34;refmt\u0026#34;)) (setf (alist-get \u0026#39;tuareg-mode apheleia-mode-alist) \u0026#39;(ocamlformat)) (setf (alist-get \u0026#39;reason-mode apheleia-mode-alist) \u0026#39;(refmt))) I don\u0026rsquo;t use lsp-ui, lsp-mode on its own does the trick combined with apheleia.\n","permalink":"https://www.joseferben.com/posts/ocaml-and-reason-on-emacs-using-lsp-mode/","summary":"\u003cp\u003eThis year I started consolidating all the major modes by using \u003ccode\u003elsp-mode\u003c/code\u003e and \u003ccode\u003eapheleia\u003c/code\u003e for all the programming language I use. The switch from OCaml\u0026rsquo;s \u003ccode\u003emerlin-mode\u003c/code\u003e was surprisingly painless.\u003c/p\u003e","title":"OCaml and Reason on Emacs using lsp-mode"},{"content":"The Django shell python manage.py shell is a very powerful tool that increases developer productivity if used correctly. With a few lines of configuration, you don\u0026rsquo;t have to leave the shell anymore to apply code changes.\nI highly recommend to use shell_plus which auto-imports your models and commonly used helpers. In order to reload the code base without leaving the shell, you need to use the autoreload extension of IPython.\nFirst, make sure you are using IPython by adding the line SHELL_PLUS = \u0026quot;ipython\u0026quot; to your Django config.\nCreate a file .iptyhon/profile_default/ipython_config.py with following content:\nc.InteractiveShellApp.extensions = [\u0026#34;autoreload\u0026#34;] c.InteractiveShellApp.exec_lines = [\u0026#34;%autoreload 2\u0026#34;] Open the shell using IPYTHONDIR=.ipython python manage.py shell_plus, make a code change and re-run the code in the shell. Magic!\n","permalink":"https://www.joseferben.com/posts/how-to-auto-reload-the-django-shell/","summary":"\u003cp\u003eThe Django shell \u003ccode\u003epython manage.py shell\u003c/code\u003e is a very powerful tool that increases developer productivity if used correctly. With a few lines of configuration, you don\u0026rsquo;t have to leave the shell anymore to apply code changes.\u003c/p\u003e","title":"How to auto-reload the Django shell"},{"content":"This is the script I am using to deploy a dokkuzied app to Dokku.\nLet\u0026rsquo;s assume we want to deploy an app called browsergame. You can us following script to do the deployment for you.\n#!/bin/sh DOKKU_HOST=\u0026#34;${DOKKU_HOST:-lettuce}\u0026#34; AWS_S3_BACKUP_PATH=\u0026#34;${AWS_S3_BACKUP_PATH:-lettuce-backups/daily}\u0026#34; echo \u0026#34;create app\u0026#34; ssh -t dokku@${DOKKU_HOST} apps:create browsergame ssh -t dokku@${DOKKU_HOST} domains:add browsergame game.joseferben.com echo \u0026#34;set up database\u0026#34; ssh -t dokku@${DOKKU_HOST} postgres:create browsergame-database ssh -t dokku@${DOKKU_HOST} postgres:link browsergame-database browsergame echo \u0026#34;set up redis\u0026#34; ssh -t dokku@${DOKKU_HOST} redis:create browsergame-redis ssh -t dokku@${DOKKU_HOST} redis:link browsergame-redis browsergame echo \u0026#34;configure app\u0026#34; ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame DJANGO_DEBUG=False ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame DJANGO_SETTINGS_MODULE=config.settings.production ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame DJANGO_SECRET_KEY=\u0026#34;$(openssl rand -base64 64 | tr -dc \u0026#39;A-HJ-NP-Za-km-z2-9\u0026#39; | head -c 64)\u0026#34; ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame DJANGO_ADMIN_URL=\u0026#34;$(openssl rand -base64 4096 | tr -dc \u0026#39;A-HJ-NP-Za-km-z2-9\u0026#39; | head -c 32)/\u0026#34; ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame MAILJET_API_KEY=${MAILJET_API_KEY} ssh -t dokku@${DOKKU_HOST} config:set --no-restart browsergame MAILJET_SECRET_KEY=${MAILJET_SECRET_KEY} echo \u0026#34;mount media files to docker volume\u0026#34; ssh root@${DOKKU_HOST} -f \u0026#39;mkdir -p /var/lib/dokku/data/storage/browsergame/\u0026#39; ssh root@${DOKKU_HOST} -f \u0026#39;chown -R dokku:dokku /var/lib/dokku/data/storage/browsergame/\u0026#39; ssh -t dokku@${DOKKU_HOST} storage:mount browsergame /var/lib/dokku/data/storage/browsergame:/storage echo \u0026#34;serve media files using nginx\u0026#34; ssh root@${DOKKU_HOST} -f \u0026#39;mkdir -p /home/dokku/browsergame/nginx.conf.d\u0026#39; ssh root@${DOKKU_HOST} -f \u0026#39;echo \u0026#34;location /media {\u0026#34; \u0026gt; /home/dokku/browsergame/nginx.conf.d/media.conf\u0026#39; ssh root@${DOKKU_HOST} -f \u0026#39;echo \u0026#34; alias /var/lib/dokku/data/storage/browsergame;\u0026#34; \u0026gt;\u0026gt; /home/dokku/browsergame/nginx.conf.d/media.conf\u0026#39; ssh root@${DOKKU_HOST} -f \u0026#39;echo \u0026#34;}\u0026#34; \u0026gt;\u0026gt; /home/dokku/browsergame/nginx.conf.d/media.conf\u0026#39; ssh root@${DOKKU_HOST} -f \u0026#39;chown -R dokku:dokku /home/dokku/browsergame/nginx.conf.d/media.conf\u0026#39; echo \u0026#34;test backup to s3\u0026#34; ssh -t dokku@${DOKKU_HOST} postgres:backup browsergame-database ${AWS_S3_BACKUP_PATH} echo \u0026#34;set up daily backups to s3\u0026#34; ssh -t dokku@${DOKKU_HOST} postgres:backup-set-encryption browsergame-database ${BACKUP_ENCRYPTION_KEY} ssh -t dokku@${DOKKU_HOST} postgres:backup-auth browsergame-database ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} ssh -t dokku@${DOKKU_HOST} postgres:backup-schedule browsergame-database @daily ${AWS_S3_BACKUP_PATH} echo \u0026#34;add dokku host as git remote\u0026#34; git remote add dokku dokku@${DOKKU_HOST}:browsergame Run the script after setting the configuration variables.\n$ export AWS_S3_BACKUP_PATH=some/s3/path $ export DOKKU_HOST=dokku-hostname $ export AWS_ACCESS_KEY_ID=\u0026lt;key\u0026gt; $ export AWS_SECRET_ACCESS_KEY=\u0026lt;secret\u0026gt; $ export BACKUP_ENCRYPTION_KEY=\u0026lt;key\u0026gt; $ export MAILJET_API_KEY=\u0026lt;key\u0026gt; $ export MAILJET_SECRET_KEY=\u0026lt;secret\u0026gt; $ ./scripts/create_dokku_app.sh $ git push dokku main ","permalink":"https://www.joseferben.com/posts/create-django-app-automatically-on-dokku/","summary":"\u003cp\u003eThis is the script I am using to deploy a \u003cem\u003edokkuzied\u003c/em\u003e app to Dokku.\u003c/p\u003e","title":"Deploy a Django app automatically to Dokku"},{"content":"It hit me that the recent decline of the Google Search result quality is making me a better programmer. Year old habits are changing. The title should be read as \u0026ldquo;How lack of Google Search is making me a better programmer\u0026rdquo;.\nMany eyes on mainstream technologies 👀 Up until October 2021, my daily driver as a programmer was an exotic mix of OCaml and NixOS. Few people are using that combination daily. This has some interesting side effects. Google Search delivers either exactly the right answers or nothing at all.\nLet\u0026rsquo;s compare the average OCaml programmer with the average Python programmer. The OCaml programmer is probably more experienced on average. Yet, in absolute numbers, there are more experienced Python programmers than OCaml programmers. Now let\u0026rsquo;s assume both groups are active in forums and discussions. There should be much more high quality content / answers about Python than OCaml. There are just so many more developers using Python than OCaml.\nThe job of Google Search is to crawl the public Python content. If Google Search does a good job, it shows me relevant quality Python content. But this is not what\u0026rsquo;s happening. Instead, I am given poorly designed GitHub or Stack Overflow clones. Or programming blogs with low effort tutorials trying to sell a course. I mean kudos to them for getting Google to list them before the content that they ripped off. This is what is called SEO garbage or SEO spam.\nIf there are enough eyes on a language or technology, there is money to be made. If there is money to be made, everything necessary will be done to catch as many eyes as possible.\nSEO spam is changing my habits I was surprised when I started using Python after years of more niche technologies. Google was not as helpful anymore as it once was for mainstream technologies.\nWith OCaml and ReasonML, I was getting used to jumping straight into the source code upon encountering a very specific issue. The chance of someone encountering the same issue with the same exotic tech stack and posting about it online was too low to even try. Yet, Googling OCaml language constructs was still efficient and it took me seconds to find good answers. Hits were usually in documentations of a bigger libraries or frameworks. I kept using Google like a programming language cheat sheet.\nI started using Django for a new project, which is a Python web framework that has a lot of eyes on it. I had to ditch Google as a cheat sheet replacement. In fact, I had to ditch Google as a long-term memory replacement.\nBack when Google released live search result suggestions that suggested queries while typing, I was excited. It worked surprisingly well, and so I started accepting suggestions for queries that were better than my own. Google knew better than myself how I wanted to search. The search results kept getting better with every year. At some point the search results were so good that usually the first search result was a hit.\nThat high quality version of Google Search made me lazy. With Google at hand, I was able to focus on higher level concepts, architecture and design. I could be sure that someone somewhere encountered a somewhat similar issue and Google would do its magic.\nDon\u0026rsquo;t get me wrong, I don\u0026rsquo;t copy and paste a code snippet without exactly understanding what it does. I admit however, that I don\u0026rsquo;t remember the archaic arguments to extract a tar file or to convert a video with ffmpeg. And that sometimes the accepted answer on Stack Overflow is useful. Especially when abandoned or very young projects lack good documentation.\nThings changed and I am starting to remember these things because wasting minutes scrolling through SEO spam is not worth it. Back when I was able to find exactly what I needed in seconds, Google Search was my general purpose cheat sheet. Today, I\u0026rsquo;ve adjusted:\nI have invested in better tooling to jump reliably into library code and to get inline documentation for dynamic languages I take notes, essentially building my own knowledge base that slowly trickles into my brain I started exploring the tools I use everyday deeper and using self documentation if available I search very specific communities using Google To put it in other words, I reduced my reliance on external information. External information outside of my own brain and external information outside of my own notes.\nA part of it is perhaps just me maturing as programmer. However, not being able to use Google Search as my general purpose cheat sheet definitely changed my habits.\nHow to Google I am not memorizing every dark corner of every standard library of every language that I am using. Nevertheless, the mental model of that Magical Input Field On White Background That Knows What I Want Before I Start Typing is gone. At least when it comes to programming and related subjects.\nAfter having tried Bing and DuckDuckGo I still use Google. Based on anecdotal evidence all the big search engines are having a hard time showing me relevant search results for my queries. I am stuck with Google but my search habits are changing.\nAppend reddit or hackernews to technological queries Use uBlacklist to block sites from showing up in Google Search Use curated blocklists to get rid of the GitHub and Stack Overflow clones Start taking notes Go back to your notes before Googling Use tooling that allows you to browse source code of libraries fast The Next Google I don\u0026rsquo;t know enough about the topic of search to write about the reasons for the outlined issues. Maybe it is the environment that makes it difficult to produce relevant search results, so even a behemoth like Google struggles. It is hard for me to believe that someone can just come out of nowhere and beat Google in general search, but who knows?\nMaybe the time has come for specific search engines? Maybe tools like GitHub Copilot will make general purpose search engines like Google less relevant for programmers?\nThank you Google Thank you Google for showing me that my brain can easily store that information that I previously deemed not worthy to store. Thank you for making me explore my everyday tools deeper and use built-in documentation where possible.\n","permalink":"https://www.joseferben.com/posts/how-google-search-is-making-me-a-better-programmer/","summary":"\u003cp\u003eIt hit me that the recent \u003ca href=\"https://twitter.com/mwseibel/status/1477701120319361026\"\u003edecline of the Google Search result quality\u003c/a\u003e is making me a better programmer. Year old habits are changing. The title should be read as \u0026ldquo;How \u003cem\u003elack of\u003c/em\u003e Google Search is making me a better programmer\u0026rdquo;.\u003c/p\u003e","title":"How Google Search is making me a better programmer"},{"content":"This is how you can selectively install packages from a specific commit or branch.\nlet # unstable channel can be used to install out-of-date packages unstable = import (builtins.fetchTarball https://github.com/nixos/nixpkgs/tarball/2310213ab2c8e00c931d60cd32f6bc1ecf1a1f15) # reuse the current configuration { config = config.nixpkgs.config; }; in environment.systemPackages = with pkgs; [ ... unstable.signal-desktop ... ] ","permalink":"https://www.joseferben.com/posts/update-nixos-packages-individually/","summary":"\u003cp\u003eThis is how you can selectively install packages from a specific commit or branch.\u003c/p\u003e","title":"Update NixOS packages individually"},{"content":"It took me three months and five attempts to fix this issues. Upon reboot two terminal panes and one Emacs instance auto started and I didn\u0026rsquo;t know why.\nEven though I run a comparably lightweight setup on my laptop, consisting mostly of XFCE as desktop manager and i3wm as window manager, it took me way too long to figure out why some things auto started.\nIn NixOS this is defined by services.xserver.displayManager.defaultSession = \u0026quot;xfce+i3\u0026quot;;. It is also possible to declaratively run commands after session creation using services.xserver.displayManager.sessionCommands. Neither of those configurations should cause two terminals and Emacs to autostart, at least it should be very obvious if that was the case. The problem must come from some mutable state that is outside of the scope of my nix configuration.\nLet\u0026rsquo;s open the XFCE Session settings using xfce4-session-settings. If there is a tab called \u0026ldquo;Saved Sessions\u0026rdquo;, we might have saved the session accidentally at some point. Now if we check the Automatically save session on logout setting on the \u0026ldquo;General\u0026rdquo; tab, it should be disabled. One would think that this means that the saved sessions are ignored when starting up. But it turns out that the setting literally means that the session is not saved anymore on logout. It will still happily load saved sessions if you have any. The fix should be quite clear at this point. The auto start can be stopped by deleting all saved sessions. Programmatically this can be done using rm ~/.cache/sessions/*.\n","permalink":"https://www.joseferben.com/posts/fix_autostart_on_xfce/","summary":"\u003cp\u003eIt took me three months and five attempts to fix this issues. Upon reboot two terminal panes and one Emacs instance auto started and I didn\u0026rsquo;t know why.\u003c/p\u003e","title":"Fix Autostart On XFCE"},{"content":"When I open Emacs to edit a Python project, the first command I run is pyvenv-activate to activate the virtualenv.\nIf I have a Python file open of that project before running pyvenv-activate, flycheck won\u0026rsquo;t work because I install my development dependencies (which are needed by the checkers) in the local virtualenv.\nLuckily pyvenv provides a hook that runs whenever a virtualenv was activated.\n(defun clear-flycheck-auto-disabled-checkers () \u0026#34;Clears any automatically disabled flycheck checkers.\u0026#34; (setq flycheck--automatically-disabled-checkers nil) (message \u0026#34;Cleared all disabled flycheck checkers\u0026#34;) (flycheck-mode) (flycheck-mode)) (setq pyvenv-post-activate-hooks (list (lambda () (clear-flycheck-auto-disabled-checkers)))) ","permalink":"https://www.joseferben.com/posts/reloading_flycheck_after_loading_virtualenv/","summary":"When I open Emacs to edit a Python project, the first command I run is pyvenv-activate to activate the virtualenv.\nIf I have a Python file open of that project before running pyvenv-activate, flycheck won\u0026rsquo;t work because I install my development dependencies (which are needed by the checkers) in the local virtualenv.\nLuckily pyvenv provides a hook that runs whenever a virtualenv was activated.\n(defun clear-flycheck-auto-disabled-checkers () \u0026#34;Clears any automatically disabled flycheck checkers.","title":"Reloading Flycheck After Loading Virtualenv"},{"content":"Improving the performance of a website can be hard. Using solid performance analysis tools is a must.\nIn order to run performance analysis using WebPageTest on a website that is protected by Cloudflare Access, you need to tell WebPageTest to authenticate.\nFirst, manually access the website by entering your email and then the confirmation code. The next request sets a cookie with the key CF_Authorization.\nThat cookie can be set as header when running the performance test. Under Script you are able to define the steps that are taken by WebPageTest during the performance test.\nsetCookie https://%HOST% CF_Authorization=\u0026lt;value\u0026gt; navigate %URL% ","permalink":"https://www.joseferben.com/posts/how_to_bypass_cloudflare_access_for_webpagetest/","summary":"\u003cp\u003eImproving the performance of a website can be hard. Using solid performance analysis tools is a must.\u003c/p\u003e","title":"How To Bypass Cloudflare Access For WebPageTest"},{"content":"I am a native German speaker who reads English articles on a daily basis. Whenever I write an English title myself, I struggle with the correct capitalization.\nLet\u0026rsquo;s take the title of this post Associated Press Stylebook Title Capitalization. To my German eyes, this looks correct. It looks correct to me, because in German, nouns and names are always capitalized. Everything else such as prepositions, adjectives and verbs are lowercase.\nA title with verbs and adjectives such as Associated Press Is Often Quoted in Local Newspapers looks a bit off. Is, Often, Quoted and Local should be lowercase according to German.\nEven though I have formed some kind of intuition for this kind of capitalization by now, I have never bothered to research whether there is some kind of rule. I semi-consciously suspected click bait as the reason for using that many capitalized words in titles and headings.\nHowever, according to the Associated Press Stylebook and Briefing on Media Law (2011 edition), the following rules for titles should be applied:\nCapitalize the first and last words. Capitalize the principal words. Capitalize prepositions and conjunctions of four letters or more. Lowercase the articles the, a, and an. There we go. 4 simple rules that are easy to follow!\n","permalink":"https://www.joseferben.com/posts/associated_press_stylebook_title_capitalization/","summary":"\u003cp\u003eI am a native German speaker who reads English articles on a daily basis. Whenever I write an English title myself, I struggle with the correct capitalization.\u003c/p\u003e","title":"Associated Press Stylebook Title Capitalization"},{"content":"I am happy to announce Schablone. Schablone is a minimal boilerplate project for Django with strong opinions and little options for customization.\nSchablone is a cookiecutter project that is heavily inspired by the famous django-cookiecutter.\nSchablone makes some pretty strong assumptions. It will speed up your app creation if you agree with them. I won\u0026rsquo;t go into details in this post because the project\u0026rsquo;s README.md does that already.\nThe deployment is documented in the generated project\u0026rsquo;s README.md.\n","permalink":"https://www.joseferben.com/posts/schablone_a_lightweight_alternative_to_cookiecutter_django/","summary":"\u003cp\u003eI am happy to announce \u003ca href=\"https://github.com/joseferben/schablone\"\u003eSchablone\u003c/a\u003e. Schablone is a minimal boilerplate project for Django with strong opinions and little options for customization.\u003c/p\u003e","title":"Schablone - A Lightweight Alternative to cookiecutter-django"},{"content":"I am using Emacs with elpy as my Python IDE. Minor issues that appeared lately made me explore other options for developing Python in Emacs. I ended up switching to a custom configuration based on anaconda-mode.\nElpy Elpy is great to get started. Having an Emacs mode that you configure with\n(use-package elpy :ensure t :init (elpy-enable)) and that sets up the basic stuff for you is amazing. Elpy showed me tools like flake8 and black.\nLately, elpy had some issues finding installed Python packages. Also syntax and error checking worked on and off.\nMy main complaint, however, is a conceptual one. I disagree with elpy\u0026rsquo;s default choice to create a global virtualenv for developer packages like linters and formatters. There is a way to configure elpy to use the local project virtualenv, but then it does not seem to work if there is no local virtualenv at all. I would like to have control over the version of the packages I use. Unfortunately this is not what elpy expects.\nThese points made me have a glance over the fence to check what else is out there. Googling python emacs reddit brought up some good discussions between real people talking about Emacs as Python IDE. I quickly stumbled upon anaconda-mode.\nI learned that both prelude-mode and Spacemacs are using anaconda-mode as their default Python mode. That was quite surprising, I assumed that elpy was the most popular Python mode (and de-facto standard) based on GitHub stars.\nOn a second thought, it makes sense for Emacs distributions to use a set of focused packaged instead of a god package like elpy. The distributions themselves aim to offer a user experience on the similar level to elpy.\nThanks to elpy I learned what I can (and should) expect from a minimal Python IDE. I was finally ready to get out there and pick and choose packages myself.\nYears ago elpy allowed me to quickly jump into a data science gig and it made me productive from day one. I still recommend elpy or full-blown Emacs distributions to beginners for this reason.\nConfiguration I don\u0026rsquo;t use a pre-configured Emacs distribution. Instead, I maintain ~/.emacs.d/init.el myself. It is a bit more work to set things up correctly, but I understand my setup reasonable well and I am quite confident that I can fix things if they go wrong.\nThis is essentially my custom Python layer that supports\nauto-complete documentation lookup finding definitions finding usages formatting buffers local virtualenv highlighting indentation and probably even more.\nI omitted the configuration of generic packages such as company or flycheck.\n(use-package python-black :ensure t :bind ((\u0026#34;C-c b\u0026#34; . python-black-buffer))) (use-package pyvenv :ensure t :config (pyvenv-mode 1)) (use-package anaconda-mode :ensure t :bind ((\u0026#34;C-c C-x\u0026#34; . next-error)) :config (require \u0026#39;pyvenv) (add-hook \u0026#39;python-mode-hook \u0026#39;anaconda-mode)) (use-package company-anaconda :ensure t :config (eval-after-load \u0026#34;company\u0026#34; \u0026#39;(add-to-list \u0026#39;company-backends \u0026#39;(company-anaconda :with company-capf)))) (use-package highlight-indent-guides :ensure t :config (add-hook \u0026#39;python-mode-hook \u0026#39;highlight-indent-guides-mode) (setq highlight-indent-guides-method \u0026#39;character)) ","permalink":"https://www.joseferben.com/posts/switching_from_elpy_to_anaconda_mode/","summary":"\u003cp\u003eI am using Emacs with \u003ca href=\"https://github.com/jorgenschaefer/elpy\"\u003eelpy\u003c/a\u003e as my Python IDE. Minor issues that appeared lately made me explore other options for developing Python in Emacs. I ended up switching to a custom configuration based on \u003ca href=\"https://github.com/pythonic-emacs/anaconda-mode\"\u003eanaconda-mode.\u003c/a\u003e\u003c/p\u003e","title":"Switching from elpy to anaconda-mode"},{"content":"Dropbox has some well known issues on Linux which made me look at alternatives. I found an alternative that I am very happy with.\nRequirements I use Getting Things Done (GTD) which requires me to take notes. In order to reduce the barrier to take notes, I need to be able to quickly pull up my note taking tool and start writing.\nReducing this barrier for note taking is one of the key insights of GTD that helped me incorporate the framework into my life. It is important that I am able to capture a thought quickly and reliably.\nReliability is important, because in order to free my brain from thought I have to make sure that I put it somewhere safe. So the notes need to be backed up.\nIn separate sessions I go through the notes and create actionable items, usually on my laptop, so I want to have the same list of notes on both machines.\nSyncthing is Ticking All the Boxes For 2-3 years I have used Dropbox. Unfortunately, it sometimes did not sync up properly when Linux woke up from sleep/hibernation, which causes quite some merge conflicts and once even data loss.\nThe alternative should be a ubiquitous as Dropbox, meaning it should be available on all major platforms.\nFortunately, I found Syncthing. It is a piece of software runs on the devices the need to be kept in sync. There is no central server involved.\nI have used it for 3 months to sync files on my phone and my laptop. So far so great, but it only works because my phone is almost always online. It kind of takes the role of a server.\nFully Replacing Dropbox With Syncthing I decided to install Syncthing on a small VPS that is online 24/7. This makes sure that all my devices get the recent versions of the files. Additionally I am using the VPS provider\u0026rsquo;s service to take snapshot backups.\nSetting Syncthing up on a VPS running Ubuntu and connecting it to my phone and my laptop took 2 minutes. You can follow a guide to install Syncthing and then open an SSH tunnel to access the GUI for setup.\nssh -L 8888:127.0.0.1:8384 root@host Open http://127.0.0.1:8888 to start the setup process.\nAdd your existing devices to the server, which will make sure that the devices know about each other.\nNo Need to Trust the Server Encryption is the feature that sold me on Syncthing. It has built in support for untrusted devices that don\u0026rsquo;t have access to the decrypted files.\nWhen you add the untrusted device, make sure to tick the Untrusted box. In the list of folders simply set a password on the trusted device for the folders that you want to share with untrusted devices.\nThe folder type on the untrusted device should be Receive Encrypted.\n","permalink":"https://www.joseferben.com/posts/replacing_dropbox_with_syncthing/","summary":"\u003cp\u003eDropbox has \u003ca href=\"https://200ok.ch/posts/2019-11-24_trigger_dropbox_on_linux_after_suspend_hibernate.html\"\u003esome well known issues\u003c/a\u003e on Linux which made me look at alternatives. I found an alternative that I am very happy with.\u003c/p\u003e","title":"Replacing Dropbox With Syncthing"},{"content":"Dokku with its Postgres plugin can be used to manage Postgres databases, this includes automated backups.\nOnce automated backups are implemented, you end up having encrypted database dumps in your backup location (which usually is some S3 storage). These are the steps needed to decrypt, unpack and apply the backups to Postgres instance managed by Dokku that the backup was created of.\ngpg --pinentry-mode=loopback --passphrase \u0026quot;\u0026lt;passphrase\u0026gt;\u0026quot; -d -o \u0026lt;decrypted\u0026gt;.tgz \u0026lt;encrypted\u0026gt;.tgz.gpg\nmkdir \u0026lt;backup\u0026gt;\ntar zxvf \u0026lt;decrypted\u0026gt;.tgz -C \u0026lt;backup\u0026gt;\ndocker exec -i \u0026lt;container_name\u0026gt; pg_restore -U admin -d dev --no-owner \u0026lt; \u0026lt;backup\u0026gt;\nI feel like these steps could be part of the great Dokku Postgres Plugin.\n","permalink":"https://www.joseferben.com/posts/recovering_dokku_postgres_backups/","summary":"\u003cp\u003e\u003ca href=\"https://dokku.com/\"\u003eDokku\u003c/a\u003e with its Postgres plugin can be used to manage Postgres databases, this includes \u003ca href=\"https://github.com/dokku/dokku-postgres#backups\"\u003eautomated backups\u003c/a\u003e.\u003c/p\u003e","title":"Recovering Dokku Postgres Backups"},{"content":"Upgrading NixOS so that it uses a different channel is simple.\nWhen you run following command as root:\nnix-channel --list You should see a list of channels. If you did not touch channels so far, you should see this:\nnixos https://nixos.org/channels/nixos-21.05 By running the following command (as root):\nnix-channel --add https://nixos.org/channels/\u0026lt;channel version\u0026gt; nixos you will remove the 21.05 channel and replace it with \u0026lt;channel version\u0026gt;.\nTo apply the change, you need to run following command:\nnixos-rebuild switch --upgrade which is going to rebuild your system with your current configuration and updated packages from the channel you switched to.\n","permalink":"https://www.joseferben.com/posts/upgrading_nixos_channels/","summary":"Upgrading NixOS so that it uses a different channel is simple.\nWhen you run following command as root:\nnix-channel --list You should see a list of channels. If you did not touch channels so far, you should see this:\nnixos https://nixos.org/channels/nixos-21.05 By running the following command (as root):\nnix-channel --add https://nixos.org/channels/\u0026lt;channel version\u0026gt; nixos you will remove the 21.05 channel and replace it with \u0026lt;channel version\u0026gt;.\nTo apply the change, you need to run following command:","title":"Upgrading NixOS Channels"},{"content":"Following the NixOS installation guide I ended up with a setup using a stable NixOS channel that just keeps giving. Sometimes however, it can be necessary to include packages from an unstable channel.\nThis is the snippet that you can use in your nix configuration if you want to install unstable neovim:\n{ config, pkgs, ... }: let unstableTarball = fetchTarball https://github.com/NixOS/nixpkgs-channels/archive/nixos-unstable.tar.gz; in { imports = [ # Include the results of the hardware scan. ./hardware-configuration.nix ]; nixpkgs.config = { packageOverrides = pkgs: with pkgs; { unstable = import unstableTarball { config = config.nixpkgs.config; }; }; }; environment.systemPackages = with pkgs; [ unstable.neovim emacs ]; } ","permalink":"https://www.joseferben.com/posts/installing_only_certain_packages_form_an_unstable_nixos_channel/","summary":"\u003cp\u003eFollowing the \u003ca href=\"https://nixos.org/manual/nixos/stable/index.html#ch-installation\"\u003eNixOS installation\u003c/a\u003e guide I ended up with a setup using a stable NixOS channel that just keeps giving. Sometimes however, it can be necessary to include packages from an unstable channel.\u003c/p\u003e","title":"Installing Only Certain Packages from Unstable on NixOS"},{"content":"Following the NixOS installation guide I ended up with a setup using a stable NixOS channel that just keeps giving. Sometimes however, it can be necessary to include packages from an unstable channel.\nThis is the snippet that you can use in your nix configuration if you want to install unstable neovim:\n{ config, pkgs, ... }: let unstableTarball = fetchTarball https://github.com/NixOS/nixpkgs-channels/archive/nixos-unstable.tar.gz; in { imports = [ # Include the results of the hardware scan. ./hardware-configuration.nix ]; nixpkgs.config = { packageOverrides = pkgs: with pkgs; { unstable = import unstableTarball { config = config.nixpkgs.config; }; }; }; environment.systemPackages = with pkgs; [ unstable.neovim emacs ]; } ","permalink":"https://www.joseferben.com/posts/installing_only_certain_packages_from_an_unstable_nixos_channel/","summary":"\u003cp\u003eFollowing the \u003ca href=\"https://nixos.org/manual/nixos/stable/index.html#ch-installation\"\u003eNixOS installation\u003c/a\u003e guide I ended up with a setup using a stable NixOS channel that just keeps giving. Sometimes however, it can be necessary to include packages from an unstable channel.\u003c/p\u003e","title":"Installing Only Certain Packages from Unstable on NixOS"},{"content":"On Linux you can set attributes to ignore files for syncing.\nTo ignore a file on Linux:\nattr -s com.dropbox.ignored -V 1 /path/to/file To unignore a file on Linux:\nattr -r com.dropbox.ignored /path/to/file ","permalink":"https://www.joseferben.com/posts/ignoring_files_with_dropbox_on_linux/","summary":"\u003cp\u003eOn Linux you can set attributes to ignore files for syncing.\u003c/p\u003e","title":"Ignoring Files with Dropbox on Linux"},{"content":"If you use urxvt to connect to your EC2 instance through Elastic Beanstalk, running\neb ssh will give you a shell where autocomplete does not work.\nFix that by running\nTERM=\u0026#39;xterm-256color\u0026#39; eb ssh ","permalink":"https://www.joseferben.com/posts/fix_bash_autocomplete_on_elastic_beanstalk_when_using_ssh/","summary":"If you use urxvt to connect to your EC2 instance through Elastic Beanstalk, running\neb ssh will give you a shell where autocomplete does not work.\nFix that by running\nTERM=\u0026#39;xterm-256color\u0026#39; eb ssh ","title":"Fixing Bash Autocompletion on Elastic Beanstalk When Using SSH"},{"content":"3 months ago I installed NixOS, i3wm and XFCE on my new Thinkpad T14 (1st Gen). It is my main machine that I use every day. In this blog post I summarize my experience with this setup.\nSetup ▗▄▄▄ ▗▄▄▄▄ ▄▄▄▖ josef@host ▜███▙ ▜███▙ ▟███▛ -------------- ▜███▙ ▜███▙▟███▛ OS: NixOS 21.05 (Okapi) x86_64 ▜███▙ ▜██████▛ Host: 20UD0013MZ ThinkPad T14 Gen 1 ▟█████████████████▙ ▜████▛ ▟▙ Kernel: 5.14.10 ▟███████████████████▙ ▜███▙ ▟██▙ Uptime: 17 hours, 35 mins ▄▄▄▄▖ ▜███▙ ▟███▛ Packages: 1534 (nix-system), 288 (nix-user) ▟███▛ ▜██▛ ▟███▛ Shell: zsh 5.8 ▟███▛ ▜▛ ▟███▛ Resolution: 1920x1080 ▟███████████▛ ▟██████████▙ DE: Xfce 4.16 ▜██████████▛ ▟███████████▛ WM: i3 ▟███▛ ▟▙ ▟███▛ Theme: Adwaita-dark [GTK2] ▟███▛ ▟██▙ ▟███▛ Icons: Rodent [GTK2] ▟███▛ ▜███▙ ▝▀▀▀▀ Terminal: urxvt ▜██▛ ▜███▙ ▜██████████████████▛ Terminal Font: Source Code Pro ▜▛ ▟████▙ ▜████████████████▛ CPU: AMD Ryzen 7 PRO 4750U with Radeon Graphics ▟██████▙ ▜███▙ GPU: AMD ATI 07:00.0 Renoir ▟███▛▜███▙ ▜███▙ Memory: 4425MiB / 15232MiB ▟███▛ ▜███▙ ▜███▙ ▝▀▀▀ ▀▀▀▀▘ ▀▀▀▘ Window Manager + Desktop Manager Coming from Arch, I was quite comfortable using i3wm as my tiling window manager. After setting it up 4 years ago I was sure I could never go back to non-tiling windows.\nHowever, I always assumed that the trade-off of using such a productive window manager was loss of convenience. With my Arch setup, I had to mount USB sticks myself, discover, pair and connect Bluetooth devices from the command line and map media keys (such as brightness and audio volume) manually in my .config/i3/config.\nLittle did I know that I was missing a Desktop Manager!\nLet me explain in Nix:\nservices.xserver.windowManager.i3.enable = true; services.xserver.desktopManager = { xterm.enable = false; xfce = { enable = true; noDesktop = true; enableXfwm = false; }; }; This is all it took to enable i3wm as my tiling Window Manager and XFCE as my Desktop Manager. Now XFCE is taking care of all the things that macOS and Windows users take for granted, such as USB stick mounting, login screen, screensaver, centralized settings and external monitor handling. I can have all that wile tiling my windows. Best of both worlds!\nTo be fair, I recall reading about using XFCE and other desktop environments with tiling window managers. But it was NixOS that made the difference between Desktop Manager and Window Manager obvious to me.\nBattery life Coming from a T470 with an external battery so large, that it was also my laptop stand, I am not that happy with the battery life of the T14. Web development, answering emails, light browsing and occasional OCaml compilation drain the battery in about 5-6 hours.\nI activated TLP using following flag:\nservices.tlp.enable = true; I suspect that I should invest some time into understanding TLP to make most of it.\nThere was an issue with the T14 draining battery while asleep. Updating to the latest BIOS fixed this issues for me.\nCPU The CPU is hands down my favorite part of the T14. It stays cool and fast. The only time when I hear the fans is when I create OCaml switches.\nIt compiles Sihl in about 15 seconds.\nScreen My second favorite part of this machine is the screen. A bright screen allows me to work in sunny places.\nSwitching my Emacs theme to the light version allows me to work directly in the sun!\nFingerprint sensor I managed to get the fingerprint sensor working with just three lines:\nservices.fprintd.enable = true; security.pam.services.login.fprintAuth = true; security.pam.services.xscreensaver.fprintAuth = true; It works well.\nTrackpad Don\u0026rsquo;t forget to enable the trackpad, which is not done by the NixOS installer by default:\nservices.xserver.libinput.enable = true; It is just like any other Thinkpad trackpad. It is good, it does its job, but it is not an Apple trackpad.\nKernel settings I adjusted the kernel settings based on some research on similar Thinkpad models and their recommended confiration.nix.\nboot.initrd.availableKernelModules = [ \u0026#34;nvme\u0026#34; \u0026#34;ehci_pci\u0026#34; \u0026#34;xhci_pci\u0026#34; \u0026#34;usb_storage\u0026#34; \u0026#34;sd_mod\u0026#34; \u0026#34;rtsx_pci_sdmmc\u0026#34; \u0026#34;thinkpad_acpi\u0026#34; ]; boot.initrd.kernelModules = [ \u0026#34;acpi_call\u0026#34; ]; boot.kernelModules = [ \u0026#34;kvm-amd\u0026#34; ]; boot.extraModulePackages = with config.boot.kernelPackages; [ acpi_call ]; With these settings sleep and hibernation work as expected when I close the lid. Not once did the T14 wake up with a closed lid.\nSummary I am very happy with this AMD Ryzen based Thinkpad. Battery life could be better, but the bright screen and silent but powerful CPU make up for it. Everything else is typical Thinkpad level quality!\n","permalink":"https://www.joseferben.com/posts/thinkpad_t14_with_nixos_and_i3wm/","summary":"\u003cp\u003e3 months ago I installed NixOS, i3wm and XFCE on my new Thinkpad T14 (1st Gen). It is my main machine that I use every day. In this blog post I summarize my experience with this setup.\u003c/p\u003e","title":"Thinkpad T14 with NixOS and I3WM"},{"content":"I would like to give a quick shout out to Aaron Aron and his blog aronwith1a.com. This has nothing to do with the fact that I get to use the tags haskell, maths and agda.\nIf you enjoy quality original content about Haskell, Maths, formal methods or riddles, go check it out. This is a post he wrote for Oxidizing Systems about Haskell.\n","permalink":"https://www.joseferben.com/posts/aronwith1a_functional_programming_math_and_riddles/","summary":"\u003cp\u003eI would like to give a quick shout out to \u003cdel\u003eAaron\u003c/del\u003e Aron and his blog \u003ca href=\"https://www.aronwith1a.com\"\u003earonwith1a.com\u003c/a\u003e. This has nothing to do with the fact that I get to use the tags \u003ca href=\"/tags/haskell\"\u003ehaskell\u003c/a\u003e, \u003ca href=\"/tags/maths/\"\u003emaths\u003c/a\u003e and \u003ca href=\"/tags/agda/\"\u003eagda\u003c/a\u003e.\u003c/p\u003e","title":"aronwith1a.com - Functional Programming, Math and Riddles"},{"content":"You sit in a cozy coffee place and you just finished your coffee. You are still not connected to the WiFi while the MacBooks and Surface devices around you have sent and received Gigabytes of data since you sat down. You are a Linux user.\nWhat are Captive Portals? Captive portals are those web pages that magically open once you connect to some public WiFi. Sometimes you have to fill in your Email address or phone number to receive a code, but often you just have to confirm some terms of services.\nOn Linux they don\u0026rsquo;t work reliably, which is extra frustrating because you know that you are one click away from having an internet connection.\nThis is a collection of links that you can try manually to land on those captive portals.\nLinks http://www.gstatic.com/generate_204 works in Starbucks ","permalink":"https://www.joseferben.com/posts/captive_portfals_in_coffee_shops_and_hotels_using_linux/","summary":"\u003cp\u003eYou sit in a cozy coffee place and you just finished your coffee. You are still not connected to the WiFi while the MacBooks and Surface devices around you have sent and received Gigabytes of data since you sat down. You are a Linux user.\u003c/p\u003e","title":"Captive Portals in Coffee Shops and Hotels using Linux"},{"content":"With following command you can print a random string of a certain length on Linux.\ntr -dc A-Za-z0-9 \u0026lt;/dev/urandom | head -c 32 ; echo \u0026#39;\u0026#39; The string is safe to use in most web contexts such as HTML forms or environment variables.\n","permalink":"https://www.joseferben.com/posts/generate_random_strings_on_linux_in_one_line/","summary":"With following command you can print a random string of a certain length on Linux.\ntr -dc A-Za-z0-9 \u0026lt;/dev/urandom | head -c 32 ; echo \u0026#39;\u0026#39; The string is safe to use in most web contexts such as HTML forms or environment variables.","title":"Generating Random Strings on Linux in One Line"},{"content":"Together with my SO we built hoarddit.com, a website that helps everyone to discover art. It allows you to virtually trade art pieces. This post describes how we spent our innovation points and why hoarddit is not an NFT.\nTable of Contents The Core Mechanics The Feeling of Collecting Is It an NFT? Boring Technology Spending Innovation Points Deliberately Summary The Core Mechanics There is a market with art pieces, currently with about 50'000 unique pieces. Some are owned by other users, but most are still owned by the bank.\nThe price for a piece is usually somewhere between 50 and 500. When you visit hoarddit.com for the first time, you are given a balance of 1000. Every 5 seconds that you spend on hoarddit, your balance gets incremented.\nYou can buy art that you like with that play money. If you can not afford a piece, just like it so you can buy it once you have enough.\nYour collection is the art that you bought and it is public. If you think you can sell a piece for a higher price, put it up for sale. Study the price chart to find trends.\nThe Feeling of Collecting When we were younger, we were always collecting something. There where different phases where everyone in primary school was collecting the same things. One year it was Yu-Gi-Oh! cards, then it was Pokemon and during the FIFA world cup it was soccer player cards.\nWith hoarddit we wanted to recreate that feeling of building up a collection. The name is an amalgamation of hoarding and reddit because that is where the art is from.\nHowever, collecting cards and collecting art on hoarddit are not the same thing. Usually there are multiple people in the world that own the same Yu-Gi-Oh! cards, but an art piece on hoarddit can have only one owner.\nWhen you buy a piece on hoarddit with your play money, you don\u0026rsquo;t actually own it. You only own the reference to the art on hoarddit. Hoarddit guarantees that no one else owns a reference to the same piece. (At least that is the promise, in reality we have to implement de-duplication.)\nOwning a reference to art. Play money. Now this rings a bell. And for some it might even raise a red flag.\nIs It an NFT? On hoarddit.com we state that the project is not an NFT. Is that really the case? Let\u0026rsquo;s examine the definition of an NFT on Wikipedia:\nA non-fungible token (NFT) is a unique and non-interchangeable unit of data stored on a digital ledger (blockchain).\nThe art pieces on hoarddit are unique units of data stored in PostgreSQL. The definition says nothing about the ledger being distributed. If there was no mention about /blockchain, maybe one could argue that hoarddit is an NFT platform by this definition.\nHowever, there is no blockchain involved, there are no tokens on distributed ledgers and most importantly, no one is getting rich over night.\nIn fact, hoarddit costs couple of bucks a month to host and there are no plans to monetize it.\nBoring Technology We chose boring and tested technology that we were familiar with.\nHoarddit is a Django app using Postgres and good old server side templates. For some dynamic components HTMX was used. The design is powered by customized Bootstrap 5. For crawling and sending emails we are using Django Q.\nEmail sending uses the generous free tier of Mailjet. The whole thing is deployed on a Hetzner VPS using Dokku.\nSentry and Uptimerobot are used for monitoring and reporting.\nThis is the final picture, but it was a process to end up with this stack. A process that involved spending some innovation points.\nUpdate 22.11.2021 We should at this point define innovation points.\nIn a project, we usually try to find a good balance between trying out new innovative (and potentially more productive) ways to do something and just getting things done. We tend to innovate/tinker/experiment too much and that\u0026rsquo;s why we give ourselves a limited amount of innovation points that that we can spend within a project.\nWhile developing hoarddit, we decided to try out new things on the UI side, so that is where we spent most of the innovation points.\nOf course that is more of a mental model and there are no actual points :)\nSpending Innovation Points Deliberately Early on during development, it became clear to us that hoarddit had some dynamic elements.\nUsers expect immediate feedback after clicking a like button. I dare you to try to refresh the page after the user leaves a like. Similar story with buying and selling art pieces. The most involved dynamic element however is infinite scrolling. Pagination by clicking next is out thanks to user metrics maximization. Even websites that have pagination in their name use infinite scrolling.\nWhich approach is the best in a situation like this? We spent some innovation points to find out.\nReact vs. AlpineJS vs. Vanilla JS vs. HTMX React Having worked with NextJS and React quite a bit, it was easy to eliminate React and the SPA approach.\nWe simply could not justify having a separate build process in a second language with a separate application lifecycle. The requirements define a limited amount of interactivity.\nOf course, there is always that voice that tells you that you could power through all the complexity for a setup that can handle arbitrary dynamic and interactive web apps.\nReact is a great tool, but it might not be the best tool for the job.\nAlpineJS We like simple and small things without dependencies. AlpineJS\u0026rsquo;s philosophy seems to honor that. AlpineJS can be used with server side templates, so we decided to give it a shot.\nThe installation was super easy. You include it in your HTML, no build process needed, 10/10.\nThe first thing we implemented was the navigation bar. This was in a early version of hoarddit before we used the Bootstrap navigation component.\nThe documentation of AlpineJS is very good, we had a working navigation that opens and closes on click within seconds. So far so good.\nNext, we wanted to implement the like feature. This involved an HTTP POST request. We did not do it inline, but separated the behavior from the markup. That went quite well, too. We were able to like and unlike art pieces within minutes.\nImplementing the buy and sell features went similarly. However, we started to notice a pattern, a pattern that we did not like.\n\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; class=\u0026#34;h-6 w-6\u0026#34; x-bind:fill=\u0026#34;liked ? \u0026#39;red\u0026#39; : \u0026#39;none\u0026#39;\u0026#34; fill=\u0026#34;{% if detail.liked %}red{% else %}none{% endif%}\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; x-bind:stroke=\u0026#34;liked ? \u0026#39;red\u0026#39; : \u0026#39;currentColor\u0026#39;\u0026#34; stroke=\u0026#34;{% if detail.liked %}red{% else %}none{% endif%}\u0026#34; \u0026gt; ... \u0026lt;/svg\u0026gt; It is true that with AlpineJS you get to use good old server side templates while you sprinkling some interactivity on top.\nFor more complex use cases however, there will be a lot of duplication. Templating logic has to be written twice, once in the server side templating language and once in AlpineJS.\nQuickly we found ourselves googling terms such as AlpineJS server side rendering hoping to find a solution on how to express the templates once and have Django and AlpineJS interpret them separately.\nA term that was used frequently a few years back was isomorphic rendering. This describes the template rendering process on the server and on the client being somewhat similar. If you use the term isomorphic rendering with this wonky definition and a Haskeller is nearby, you did not read it here!\nAlpineJS was a joy to use for little stateless elements like the navigation that opens and closes. Our use case seems to be too complex for it however.\nVanilla JavaScript In a moment of minimalism we decided to scrap all frameworks and libraries. What do they give us anyway apart from dependency hell and layers and layers of abstraction?\nAfter implementing the navigation in JavaScript, it became clear that we did not want to further go down this route.\nfunction loadNavigation() { const closeButton = document.getElementById(\u0026#34;navigation-close-button\u0026#34;); const openButton = document.getElementById(\u0026#34;navigation-open-button\u0026#34;); const navigationOpen = document.getElementById(\u0026#34;navigation-open\u0026#34;); const navigationClosed = document.getElementById(\u0026#34;navigation-closed\u0026#34;); closeButton.addEventListener(\u0026#39;click\u0026#39;, function (event) { navigationOpen.classList.add(\u0026#34;d-none\u0026#34;); navigationClosed.classList.remove(\u0026#34;d-none\u0026#34;); }); openButton.addEventListener(\u0026#39;click\u0026#39;, function (event) { navigationOpen.classList.remove(\u0026#34;d-none\u0026#34;); navigationClosed.classList.add(\u0026#34;d-none\u0026#34;); }); } Using JavaScript improved the dependency situation compared to AlpineJS, because there was one dependency less. But the gain was minimal, AlpineJS is just very lightweight and easy to include.\nAll of the sudden we had references to DOM nodes including the maintenance burden they bring. No thanks!\nHTMX The next piece of technology we tried out was HTMX, which further reduced the amount of JavaScript we had to write.\nMeanwhile we started using the navigation component of Bootstrap 5, so there was no custom JavaScript to write.\nFeeling empowered by the good documentation and useful examples on the HTMX website, we decided to implement infinite scrolling. Half an hour later, we had the first working version.\nIn a rush of euphoria, we decided to HTMXize the whole project; liking, unliking, selling, buying and updating the balance. It went quite well, we implemented all those features without duplicating code in the backend and on the client.\nThis is how the markup of the like feature looks like:\n\u0026lt;button id=\u0026#34;piece-detail-like-toggle-button\u0026#34; hx-post=\u0026#34;{% url \u0026#39;core:like\u0026#39; details.piece.id %}\u0026#34; hx-swap=\u0026#34;outerHtml\u0026#34; hx-target=\u0026#34;#piece-detail-dynamic\u0026#34; type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-outline-secondary\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;bi bi-suit-heart\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/button\u0026gt; HTMX truly feels like an extension of HTML, something that should have been standardized and included in HTML a while ago.\nThere seems to be a movement towards server side rendering + smartness like Liveview, Blazor, Livewire and Hotwire.\nTo this date, HTMX is powering the dynamic and interactive elements of hoarddit. There is a bit of custom JavaScript that allows the user to like a picture by double tap or the rendering of price charts.\nRunning Out of Innovation Points This blog post was written with the power of hindsight and we make it sound like we spent innovation points very precisely in a controlled way. That was not the case.\nIn search of simplicity and boredom, we initially deployed hoarddit to AWS Elastic Beanstalk. This worked quite well until we needed workers. The Beanstalk way to do this is to use a separate worker environment within the same Beanstalk project.\nBack then, hoarddit used huey which we were not able to get working on Beanstalk. So we kept trying out queues until we found one that worked. It felt weird to not have an identical setup locally, but the deployment to Beanstalk was quite ergonomic.\nAnd then we needed a staging environment. On Beanstalk, this means a second project with the same environments, which means twice the costs. The Beanstalk deployments were sometimes hanging for no obvious reason, so we decided to self host. We did not need Elastic Load Balancer anyway.\nWe got ourselves a small VPS at Hetzner and deployed everything with Dokku. Dokku is a great piece of simple technology that deserves its own blog post.\nSummary Finishing up a pet project that was built over several months with a couple of hours every other weekend feels amazing.\nHoarddit has a few active users and the early feedback we got is positive. Many requested social features that make things others do more visible.\nIf you have any feedback or feature request, don\u0026rsquo;t hesitate to reach out by social media or email at hello@hoarddit.com.\n","permalink":"https://www.joseferben.com/posts/hoarddit_a_website_to_discover_art/","summary":"\u003cp\u003eTogether with \u003ca href=\"https://www.instagram.com/tylmarple/\"\u003emy SO\u003c/a\u003e we built \u003ca href=\"https://www.hoarddit.com\"\u003ehoarddit.com\u003c/a\u003e, a website that helps everyone to discover art. It allows you to virtually trade art pieces. This post describes how we spent our innovation points and why hoarddit is not an NFT.\u003c/p\u003e","title":"Hoarddit - A Website to Discover Art"},{"content":"In order to play Anno 1404 on Proton, follow these steps.\nInstall DirectX WINEPREFIX=/home/\u0026lt;user\u0026gt;/.local/share/Steam/steamapps/compatdata/33350/pfx wine /home/\u0026lt;user\u0026gt;/.local/share/Steam/steamapps/common/Anno\\ 1404/DirectX/DXSETUP.exe Edit Engine.ini file The content of the file\n/home/\u0026lt;user\u0026gt;/.local/share/Steam/steamapps/common/Anno\\ 1404/Engine.ini \u0026lt;InitFile\u0026gt; \u0026lt;DirectXVersion\u0026gt;9\u0026lt;/DirectXVersion\u0026gt; \u0026lt;UbiSurveyTime\u0026gt;-1\u0026lt;/UbiSurveyTime\u0026gt; \u0026lt;UbiSurveyTimeStatus\u0026gt;2\u0026lt;/UbiSurveyTimeStatus\u0026gt; \u0026lt;/InitFile\u0026gt; ","permalink":"https://www.joseferben.com/posts/play_anno_1404_venice_on_proton/","summary":"\u003cp\u003eIn order to play Anno 1404 on \u003ca href=\"https://www.protondb.com/app/33350\"\u003eProton\u003c/a\u003e, follow these steps.\u003c/p\u003e","title":"Playing Anno 1404 Venice on Proton"},{"content":"Install Dropbox using NixOS in your preferred way.\nAt the top of your i3wm configuration file add\nexec dropbox \u0026amp; ","permalink":"https://www.joseferben.com/posts/autostart_dropbox_on_nixos/","summary":"\u003cp\u003eInstall Dropbox using NixOS in your preferred way.\u003c/p\u003e","title":"Autostart Dropbox on NixOS using I3WM"},{"content":"When hosting Django applications on AWS Elastic Beanstalk, it is often required to run commands on the server to do some maintenance tasks.\nBeanstalk provisions EC2 instances which are running the web server processes. Django commands need to run in the same environment.\nYou might want to fix your shell first.\nAfter connecting using SSH with\neb ssh load the environment variables using\nexport $(cat /opt/elasticbeanstalk/deployment/env | xargs) then load the Python environment using\nsource /var/app/venv and finally run your Django command\npython /var/app/current/manage.py runserver ","permalink":"https://www.joseferben.com/posts/running_django_commands_on_aws_elastic_beanstalk/","summary":"\u003cp\u003eWhen hosting Django applications on AWS Elastic Beanstalk, it is often required to run commands on the server to do some maintenance tasks.\u003c/p\u003e","title":"Running Django Commands on AWS Elastic Beanstalk"}]